{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration Based Methods - word2vec\n",
    "Iteration-based methods capture cooccurrence of words one at a time instead of capturing all cooccurence \n",
    "counts directly like in SVD method. Word2vec is actually a software package includes 2 algorithms (CBOW, Skip-gram) \n",
    "and 2 training methods (negative sampling and hierarchical softmax).  \n",
    "\n",
    "### continuous bag-of-words (CBOW)\n",
    "Predicts a center word from the surroudning context in terms of word vectors. V is the input word matrix,\n",
    "U is the output word matrix. For each word we want to learn both the input and output vectors.  \n",
    "<br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/CBOW.png?raw=true\" width=30% /><br> \n",
    "- Generate one hot word vectors for the input context with size **2m**:   \n",
    "**(x(c-m),...,x(c-1),x(c+1),...,x(c+m) with shape (V, 1))**  \n",
    "- Get the embedded word vectors for the context: (**n** is the embedded dimension)  \n",
    "**(v(c-m)=Vx(c-m),v(c-m+1)=Vx(c-m+1),...,v(c+m)=Vx(c+m) with shape (n, 1))**  \n",
    "- Average all context word vectors, get **v(hat)**.  \n",
    "- Generate a score vector **z=Uv(hat)(with shape(V, 1))**.  \n",
    "- Turn the scores into probabilities using **softmax** function.  \n",
    "- Calculate the cross entropy **H(y(hat), y)** between the predicted probability distribution and \n",
    "the one hot vector of the true center word.  \n",
    "<br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/CBOW_obj.png?raw=true\" width=60% /><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "corpus = open(path).readlines()[:300]\n",
    "corpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17897.1324615\n",
      "1 16611.4819975\n",
      "2 16054.5737875\n",
      "3 15975.4586413\n",
      "4 16078.7989137\n",
      "5 16168.2691295\n",
      "6 16193.6849765\n",
      "7 16191.0186955\n",
      "8 16183.2132714\n",
      "9 16174.5272493\n"
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0.8029883503913879),\n",
       " ('her', 0.6420163512229919),\n",
       " ('i', 0.6390146613121033),\n",
       " ('it', 0.6209530830383301),\n",
       " ('this', 0.6207232475280762),\n",
       " ('my', 0.6043308973312378),\n",
       " ('you', 0.5718163847923279),\n",
       " ('to', 0.5632857084274292),\n",
       " ('about', 0.5545670986175537),\n",
       " ('down', 0.5478866696357727)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram (SG)  \n",
    "Predicts the context words from a center word. V is the input word matrix,\n",
    "U is the output word matrix. For each word we want to learn both the input and output vectors.  \n",
    "<br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/SG.png?raw=true\" width=30% /><br> \n",
    "- Generate one hot word vector for the center word **x, with shape(V, 1)**   \n",
    "- Get the embedded word vector for the center word: **v(c)=Vx with shape(n, 1)**   \n",
    "- Generate a score vector **z=Uv(c)(with shape(V, 1))**.  \n",
    "- Turn the scores into probabilities using **softmax** function.  \n",
    "- Calculate the cross entropy **H(y(hat), y)** between the predicted probability distribution and \n",
    "the one hot vector of the true context words.  \n",
    "<br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/SG_obj.png?raw=true\" width=60% />\n",
    "\n",
    "### Training methods\n",
    "- **Negative Sampling**  \n",
    "For softmax function, the summation over the whole vocabulary(V) is computationally huge. \n",
    "A simple idea is to approximate it by sampling the negative samples.  \n",
    "We can build a new objective function that tries to maximize the probability of a word \n",
    "and context being in the corpus data if it indeed is, and maximize the probability of a word \n",
    "and context not being in the corpus data if it indeed is not. We take a simple maximum \n",
    "likelihood approach of these two probabilities.  \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/NS_01.png?raw=true\" width=60% /><br>  \n",
    "Maximizing the likelihood is the same as minimizing the negative log likelihood. \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/NS_02.png?raw=true\" width=60% /><br>  \n",
    "We can generate the negative half of cost by randomly sampling this negative from the word bank.\n",
    "Regarding the sampling function **P(W)**, what seems to work best is the Unigram Model raised to the power of 3/4.  \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/NS_03.png?raw=true\" width=30% /><br> \n",
    "    - Negative Sampling objective function for **CBOW**  \n",
    "        <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/NS_CBOW.png?raw=true\" width=40% /><br>  \n",
    "    - Negative Sampling objective function for **Skip-gram**  \n",
    "        <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/NS_SG.png?raw=true\" width=45% />\n",
    "    \n",
    "- **Hierarchical Softmax**  \n",
    "Hierarchical softmax uses a binary tree (like a binary Huffman tree, which assigns frequent\n",
    "words shorter paths in the tree) to represent all words in the vocabulary.\n",
    "Each leaf of the tree is a word, and there is a unique path from root to leaf.\n",
    "In this model, **there is no output representation for words.**\n",
    "Instead, each node of the graph (except the root and the leaves) is associated to a vector \n",
    "that the model is going to learn.  \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/HS.png?raw=true\" width=50%/><br>   \n",
    "Let **L(w)** be the number of nodes in the path from the root to the leaf **w**.\n",
    "For instance, **L(w2)** in the above figure is 3.\n",
    "Let’s write **n(w, i)** as the **i-th** node on this path with associated vector **vn(w,i)**.\n",
    "So **n(w, 1)** is the root, while **n(w, L(w))** is the father of **w**.\n",
    "Now for each inner node **n**, we arbitrarily choose one of its\n",
    "children and call it **ch(n)** (e.g. always the left node). Then, we can\n",
    "compute the probability as  \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/HS_PROB.png?raw=true\" width=60%/><br>   \n",
    "For example, taking w2 in the above figure, we must take two left edges and then a right \n",
    "edge to reach w2 from the root, so  \n",
    "    <br><img src=\"https://github.com/SauceCat/NLP-Playground/blob/master/word_embedding/images/HS_example.png?raw=true\" width=60%/><br>   \n",
    "To train the model, our goal is still to minimize the negative log likelihood **-log P(w|wi)**.\n",
    "But instead of updating output vectors per word, we update the vectors of the nodes in the binary \n",
    "tree that are in the path from root to leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape\n",
    "from IPython.display import SVG\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from gensim.models.doc2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('alice.txt', origin=\"http://www.gutenberg.org/cache/epub/11/pg11.txt\")\n",
    "corpus = open(path).readlines()[0:200]\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word] )\n",
    "                    labels.append(words[i])\n",
    "\n",
    "            x = np.array(in_words,dtype=np.int32)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 336.32 296.00\" width=\"336pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-292 332.321,-292 332.321,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4567794968 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4567794968</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 328.321,-287.5 328.321,-243.5 0,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-261.3\">embedding_input_3 (InputLayer)</text>\n",
       "<polyline fill=\"none\" points=\"203.004,-243.5 203.004,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230.838\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"203.004,-265.5 258.673,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230.838\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"258.673,-243.5 258.673,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.497\" y=\"-272.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"258.673,-265.5 328.321,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293.497\" y=\"-250.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 4567794856 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4567794856</title>\n",
       "<polygon fill=\"none\" points=\"2.71387,-162.5 2.71387,-206.5 325.607,-206.5 325.607,-162.5 2.71387,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.502\" y=\"-180.3\">embedding_3 (Embedding)</text>\n",
       "<polyline fill=\"none\" points=\"172.29,-162.5 172.29,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.125\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"172.29,-184.5 227.959,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"200.125\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"227.959,-162.5 227.959,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.783\" y=\"-191.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"227.959,-184.5 325.607,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276.783\" y=\"-169.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 4567794968&#45;&gt;4567794856 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4567794968-&gt;4567794856</title>\n",
       "<path d=\"M164.161,-243.329C164.161,-235.183 164.161,-225.699 164.161,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"167.661,-216.729 164.161,-206.729 160.661,-216.729 167.661,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4567794352 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4567794352</title>\n",
       "<polygon fill=\"none\" points=\"21,-81.5 21,-125.5 307.321,-125.5 307.321,-81.5 21,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87.502\" y=\"-99.3\">reshape_3 (Reshape)</text>\n",
       "<polyline fill=\"none\" points=\"154.004,-81.5 154.004,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.838\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"154.004,-103.5 209.673,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181.838\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"209.673,-81.5 209.673,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.497\" y=\"-110.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"209.673,-103.5 307.321,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.497\" y=\"-88.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 4567794856&#45;&gt;4567794352 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4567794856-&gt;4567794352</title>\n",
       "<path d=\"M164.161,-162.329C164.161,-154.183 164.161,-144.699 164.161,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"167.661,-135.729 164.161,-125.729 160.661,-135.729 167.661,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4567757152 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4567757152</title>\n",
       "<polygon fill=\"none\" points=\"39.6587,-0.5 39.6587,-44.5 288.663,-44.5 288.663,-0.5 39.6587,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94.502\" y=\"-18.3\">dense_3 (Dense)</text>\n",
       "<polyline fill=\"none\" points=\"149.345,-0.5 149.345,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.18\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"149.345,-22.5 205.014,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.18\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"205.014,-0.5 205.014,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.838\" y=\"-29.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"205.014,-22.5 288.663,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.838\" y=\"-7.3\">(None, 573)</text>\n",
       "</g>\n",
       "<!-- 4567794352&#45;&gt;4567757152 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4567794352-&gt;4567757152</title>\n",
       "<path d=\"M164.161,-81.3294C164.161,-73.1826 164.161,-63.6991 164.161,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"167.661,-54.729 164.161,-44.729 160.661,-54.729 167.661,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))\n",
    "SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10880.5715966\n",
      "1 10205.9945803\n",
      "2 10074.3077066\n",
      "3 9965.59868503\n",
      "4 9854.57731271\n",
      "5 9750.60724521\n",
      "6 9657.09119558\n",
      "7 9572.85030174\n",
      "8 9495.95489979\n",
      "9 9424.25686908\n"
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim)]))\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 0.42469197511672974),\n",
       " ('too', 0.4138217866420746),\n",
       " ('whether', 0.3842674493789673),\n",
       " ('wonderland', 0.368587851524353),\n",
       " ('in', 0.36720913648605347),\n",
       " ('that', 0.3651423156261444),\n",
       " ('adventures', 0.3534693419933319),\n",
       " ('labelled', 0.31480032205581665),\n",
       " ('yes', 0.30488651990890503),\n",
       " ('got', 0.29601991176605225)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['alice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV-DM\n",
    "\n",
    "Every paragraph is mapped to a unique vector, represented by a column in matrix **D** and every word is also mapped to a unique vector, represented by a column in matrix **W**. The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context.  \n",
    "\n",
    "In other words, when predicting the next word given a number of context words, the paragraph vectors are also asked to contribute to the prediction task of the next word given many contexts sampled from the paragraph. **So the major different between this model and CBOW model is, here h is constructed from W and D.**  \n",
    "<br><center><img src='https://github.com/SauceCat/NLP-Playground/blob/master/doc2vec/images/word2vec.png?raw=true' width=50%></center><br>\n",
    "\n",
    "The paragraph token can be thought of as another word. It acts as a memory that remembers what is missing from the current context – or the topic of the paragraph. For this reason, this model is named **Distributed Memory Model of Paragraph Vectors (PV-DM)**.  \n",
    "<br><center><img src='https://github.com/SauceCat/NLP-Playground/blob/master/doc2vec/images/PV_DM.png?raw=true' width=50%></center><br><br>\n",
    "\n",
    "The contexts are fixed-length and sampled from a sliding window over the paragraph. The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. The word vector matrix W, how-ever, is shared across paragraphs.  \n",
    "\n",
    "At prediction time, one needs to perform an inference step to compute the paragraph vector for a new paragraph. This is also obtained by gradient descent. In this step, the parameters for the rest of the model, the word vectors W and the softmax weights, are fixed.  \n",
    "\n",
    ">Paragraph vectors also address some of the key weaknesses of bag-of-words models.\n",
    ">1. They inherit an important property of the word vectors: the semantics of the words.  \n",
    ">2. They take into consideration the word order, at least in a small context.  \n",
    "\n",
    "## PV-DBOW\n",
    "Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output given the Paragraph Vector. This technique is named as **Distributed Bag of Words (PV-DBOW)**, which is similar to the skip-gram model of word2vec.  \n",
    "<br><center><img src='https://github.com/SauceCat/NLP-Playground/blob/master/doc2vec/images/PV_DBOW.png?raw=true' width=50%></center><br>\n",
    "  \n",
    "Compared with the distributed memory model, this model requires to store less data. We only need to store the softmax weights as opposed to both softmax weights and word vectors in the previous model.\n",
    "\n",
    "\n",
    "## In practice\n",
    "PV-DM alone usually works well for most tasks (with state-of-art performances), but its combination with PV-DBOW is usually more consistent across many tasks that we try and therefore strongly recommended."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
