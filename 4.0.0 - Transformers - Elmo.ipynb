{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# ELMo: Context Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we’re using this GloVe representation, then the word “stick” would be represented by this vector no-matter what the context was. “Wait a minute” said a number of NLP researchers (Peters et. al., 2017, McCann et. al., 2017, and yet again Peters et. al., 2018 in the ELMo paper ), “stick”” has multiple meanings depending on where it’s used. Why not give it an embedding based on the context it’s used in – to both capture the word meaning in that context as well as other contextual information?”. And so, contextualized word-embeddings were born."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/elmo-embedding-robin-williams.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextualized word-embeddings can give words different embeddings based on the meaning they carry in the context of the sentence. Also, RIP Robin Williams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/elmo-word-embedding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.\n",
    "\n",
    "What’s ELMo’s secret?\n",
    "\n",
    "ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Bert-language-modeling.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.\n",
    "\n",
    "ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/elmo-forward-backward-language-model-embedding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/elmo-embedding.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Input, SpatialDropout1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, Activation\n",
    "from keras.layers import Lambda, Embedding, Conv2D, GlobalMaxPool1D\n",
    "from keras.layers import add, concatenate\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.utils import to_categorical\n",
    "from tachles import TimestepDropout, Highway, Camouflage, SampledSoftmax, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_DIR = './'\n",
    "DATA_SET_DIR = os.path.join(DATA_BASE_DIR, 'dataset')\n",
    "MODELS_DIR = os.path.join(DATA_BASE_DIR, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(object):\n",
    "    def __init__(self, parameters):\n",
    "        self._model = None\n",
    "        self._elmo_model = None\n",
    "        self.parameters = parameters\n",
    "        self.compile_elmo()\n",
    "\n",
    "    def __del__(self):\n",
    "        K.clear_session()\n",
    "        del self._model\n",
    "\n",
    "    def char_level_token_encoder(self):\n",
    "        charset_size = self.parameters['charset_size']\n",
    "        char_embedding_size = self.parameters['char_embedding_size']\n",
    "        token_embedding_size = self.parameters['hidden_units_size']\n",
    "        n_highway_layers = self.parameters['n_highway_layers']\n",
    "        filters = self.parameters['cnn_filters']\n",
    "        token_maxlen = self.parameters['token_maxlen']\n",
    "\n",
    "        # Input Layer, word characters (samples, words, character_indices)\n",
    "        inputs = Input(shape=(None, token_maxlen,), dtype='int32')\n",
    "        # Embed characters (samples, words, characters, character embedding)\n",
    "        embeds = Embedding(input_dim=charset_size, output_dim=char_embedding_size)(inputs)\n",
    "        token_embeds = []\n",
    "        # Apply multi-filter 2D convolutions + 1D MaxPooling + tanh\n",
    "        for (window_size, filters_size) in filters:\n",
    "            convs = Conv2D(filters=filters_size, kernel_size=[window_size, char_embedding_size], strides=(1, 1),\n",
    "                           padding=\"same\")(embeds)\n",
    "            convs = TimeDistributed(GlobalMaxPool1D())(convs)\n",
    "            convs = Activation('tanh')(convs)\n",
    "            convs = Camouflage(mask_value=0)(inputs=[convs, inputs])\n",
    "            token_embeds.append(convs)\n",
    "        token_embeds = concatenate(token_embeds)\n",
    "        # Apply highways networks\n",
    "        for i in range(n_highway_layers):\n",
    "            token_embeds = TimeDistributed(Highway())(token_embeds)\n",
    "            token_embeds = Camouflage(mask_value=0)(inputs=[token_embeds, inputs])\n",
    "        # Project to token embedding dimensionality\n",
    "        token_embeds = TimeDistributed(Dense(units=token_embedding_size, activation='linear'))(token_embeds)\n",
    "        token_embeds = Camouflage(mask_value=0)(inputs=[token_embeds, inputs])\n",
    "\n",
    "        token_encoder = Model(inputs=inputs, outputs=token_embeds, name='token_encoding')\n",
    "        return token_encoder\n",
    "\n",
    "    def compile_elmo(self, print_summary=False):\n",
    "        \"\"\"\n",
    "        Compiles a Language Model RNN based on the given parameters\n",
    "        \"\"\"\n",
    "\n",
    "        if self.parameters['token_encoding'] == 'word':\n",
    "            # Train word embeddings from scratch\n",
    "            word_inputs = Input(shape=(None,), name='word_indices', dtype='int32')\n",
    "            embeddings = Embedding(self.parameters['vocab_size'], self.parameters['hidden_units_size'], trainable=True, name='token_encoding')\n",
    "            inputs = embeddings(word_inputs)\n",
    "\n",
    "            # Token embeddings for Input\n",
    "            drop_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(inputs)\n",
    "            lstm_inputs = TimestepDropout(self.parameters['word_dropout_rate'])(drop_inputs)\n",
    "\n",
    "            # Pass outputs as inputs to apply sampled softmax\n",
    "            next_ids = Input(shape=(None, 1), name='next_ids', dtype='float32')\n",
    "            previous_ids = Input(shape=(None, 1), name='previous_ids', dtype='float32')\n",
    "        elif self.parameters['token_encoding'] == 'char':\n",
    "            # Train character-level representation\n",
    "            word_inputs = Input(shape=(None, self.parameters['token_maxlen'],), dtype='int32', name='char_indices')\n",
    "            inputs = self.char_level_token_encoder()(word_inputs)\n",
    "\n",
    "            # Token embeddings for Input\n",
    "            drop_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(inputs)\n",
    "            lstm_inputs = TimestepDropout(self.parameters['word_dropout_rate'])(drop_inputs)\n",
    "\n",
    "            # Pass outputs as inputs to apply sampled softmax\n",
    "            next_ids = Input(shape=(None, 1), name='next_ids', dtype='float32')\n",
    "            previous_ids = Input(shape=(None, 1), name='previous_ids', dtype='float32')\n",
    "\n",
    "        # Reversed input for backward LSTMs\n",
    "        re_lstm_inputs = Lambda(function=ELMo.reverse)(lstm_inputs)\n",
    "        mask = Lambda(function=ELMo.reverse)(drop_inputs)\n",
    "\n",
    "        # Forward LSTMs\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            if self.parameters['cuDNN']:\n",
    "                lstm = CuDNNLSTM(units=self.parameters['lstm_units_size'], return_sequences=True,\n",
    "                                 kernel_constraint=MinMaxNorm(-1*self.parameters['cell_clip'],\n",
    "                                                              self.parameters['cell_clip']),\n",
    "                                 recurrent_constraint=MinMaxNorm(-1*self.parameters['cell_clip'],\n",
    "                                                                 self.parameters['cell_clip']))(lstm_inputs)\n",
    "            else:\n",
    "                lstm = LSTM(units=self.parameters['lstm_units_size'], return_sequences=True, activation=\"tanh\",\n",
    "                            recurrent_activation='sigmoid',\n",
    "                            kernel_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                         self.parameters['cell_clip']),\n",
    "                            recurrent_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                            self.parameters['cell_clip'])\n",
    "                            )(lstm_inputs)\n",
    "            lstm = Camouflage(mask_value=0)(inputs=[lstm, drop_inputs])\n",
    "            # Projection to hidden_units_size\n",
    "            proj = TimeDistributed(Dense(self.parameters['hidden_units_size'], activation='linear',\n",
    "                                         kernel_constraint=MinMaxNorm(-1 * self.parameters['proj_clip'],\n",
    "                                                                      self.parameters['proj_clip'])\n",
    "                                         ))(lstm)\n",
    "            # Merge Bi-LSTMs feature vectors with the previous ones\n",
    "            lstm_inputs = add([proj, lstm_inputs], name='f_block_{}'.format(i + 1))\n",
    "            # Apply variational drop-out between BI-LSTM layers\n",
    "            lstm_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(lstm_inputs)\n",
    "\n",
    "        # Backward LSTMs\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            if self.parameters['cuDNN']:\n",
    "                re_lstm = CuDNNLSTM(units=self.parameters['lstm_units_size'], return_sequences=True,\n",
    "                                    kernel_constraint=MinMaxNorm(-1*self.parameters['cell_clip'],\n",
    "                                                                 self.parameters['cell_clip']),\n",
    "                                    recurrent_constraint=MinMaxNorm(-1*self.parameters['cell_clip'],\n",
    "                                                                    self.parameters['cell_clip']))(re_lstm_inputs)\n",
    "            else:\n",
    "                re_lstm = LSTM(units=self.parameters['lstm_units_size'], return_sequences=True, activation='tanh',\n",
    "                               recurrent_activation='sigmoid',\n",
    "                               kernel_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                            self.parameters['cell_clip']),\n",
    "                               recurrent_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                               self.parameters['cell_clip'])\n",
    "                               )(re_lstm_inputs)\n",
    "            re_lstm = Camouflage(mask_value=0)(inputs=[re_lstm, mask])\n",
    "            # Projection to hidden_units_size\n",
    "            re_proj = TimeDistributed(Dense(self.parameters['hidden_units_size'], activation='linear',\n",
    "                                            kernel_constraint=MinMaxNorm(-1 * self.parameters['proj_clip'],\n",
    "                                                                         self.parameters['proj_clip'])\n",
    "                                            ))(re_lstm)\n",
    "            # Merge Bi-LSTMs feature vectors with the previous ones\n",
    "            re_lstm_inputs = add([re_proj, re_lstm_inputs], name='b_block_{}'.format(i + 1))\n",
    "            # Apply variational drop-out between BI-LSTM layers\n",
    "            re_lstm_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(re_lstm_inputs)\n",
    "\n",
    "        # Reverse backward LSTMs' outputs = Make it forward again\n",
    "        re_lstm_inputs = Lambda(function=ELMo.reverse, name=\"reverse\")(re_lstm_inputs)\n",
    "\n",
    "        # Project to Vocabulary with Sampled Softmax\n",
    "        sampled_softmax = SampledSoftmax(num_classes=self.parameters['vocab_size'],\n",
    "                                         num_sampled=int(self.parameters['num_sampled']),\n",
    "                                         tied_to=embeddings if self.parameters['weight_tying'] else None)\n",
    "        outputs = sampled_softmax([lstm_inputs, next_ids])\n",
    "        re_outputs = sampled_softmax([re_lstm_inputs, previous_ids])\n",
    "\n",
    "        self._model = Model(inputs=[word_inputs, next_ids, previous_ids],\n",
    "                            outputs=[outputs, re_outputs])\n",
    "        self._model.compile(optimizer=Adagrad(lr=self.parameters['lr'], clipvalue=self.parameters['clip_value']),\n",
    "                            loss=None)\n",
    "        if print_summary:\n",
    "            self._model.summary()\n",
    "\n",
    "    def train(self, train_data, valid_data):\n",
    "\n",
    "        # Add callbacks (early stopping, model checkpoint)\n",
    "        weights_file = os.path.join(MODELS_DIR, \"elmo_best_weights.hdf5\")\n",
    "        save_best_model = ModelCheckpoint(filepath=weights_file, monitor='val_loss', verbose=1,\n",
    "                                          save_best_only=True, mode='auto')\n",
    "        early_stopping = EarlyStopping(patience=self.parameters['patience'], restore_best_weights=True)\n",
    "\n",
    "        t_start = time.time()\n",
    "\n",
    "        # Fit Model\n",
    "        self._model.fit_generator(train_data,\n",
    "                                  validation_data=valid_data,\n",
    "                                  epochs=self.parameters['epochs'],\n",
    "                                  workers=self.parameters['n_threads']\n",
    "                                  if self.parameters['n_threads'] else os.cpu_count(),\n",
    "                                  use_multiprocessing=True\n",
    "                                  if self.parameters['multi_processing'] else False,\n",
    "                                  callbacks=[save_best_model])\n",
    "\n",
    "        print('Training took {0} sec'.format(str(time.time() - t_start)))\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "\n",
    "        def unpad(x, y_true, y_pred):\n",
    "            y_true_unpad = []\n",
    "            y_pred_unpad = []\n",
    "            for i, x_i in enumerate(x):\n",
    "                for j, x_ij in enumerate(x_i):\n",
    "                    if x_ij == 0:\n",
    "                        y_true_unpad.append(y_true[i][:j])\n",
    "                        y_pred_unpad.append(y_pred[i][:j])\n",
    "                        break\n",
    "            return np.asarray(y_true_unpad), np.asarray(y_pred_unpad)\n",
    "\n",
    "        # Generate samples\n",
    "        x, y_true_forward, y_true_backward = [], [], []\n",
    "        for i in range(len(test_data)):\n",
    "            test_batch = test_data[i][0]\n",
    "            x.extend(test_batch[0])\n",
    "            y_true_forward.extend(test_batch[1])\n",
    "            y_true_backward.extend(test_batch[2])\n",
    "        x = np.asarray(x)\n",
    "        y_true_forward = np.asarray(y_true_forward)\n",
    "        y_true_backward = np.asarray(y_true_backward)\n",
    "\n",
    "        # Predict outputs\n",
    "        y_pred_forward, y_pred_backward = self._model.predict([x, y_true_forward, y_true_backward])\n",
    "\n",
    "        # Unpad sequences\n",
    "        y_true_forward, y_pred_forward = unpad(x, y_true_forward, y_pred_forward)\n",
    "        y_true_backward, y_pred_backward = unpad(x, y_true_backward, y_pred_backward)\n",
    "\n",
    "        # Compute and print perplexity\n",
    "        print('Forward Langauge Model Perplexity: {}'.format(ELMo.perplexity(y_pred_forward, y_true_forward)))\n",
    "        print('Backward Langauge Model Perplexity: {}'.format(ELMo.perplexity(y_pred_backward, y_true_backward)))\n",
    "\n",
    "    def wrap_multi_elmo_encoder(self, print_summary=False, save=False):\n",
    "        \"\"\"\n",
    "        Wrap ELMo meta-model encoder, which returns an array of the 3 intermediate ELMo outputs\n",
    "        :param print_summary: print a summary of the new architecture\n",
    "        :param save: persist model\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        elmo_embeddings = list()\n",
    "        elmo_embeddings.append(concatenate([self._model.get_layer('token_encoding').output, self._model.get_layer('token_encoding').output],\n",
    "                                           name='elmo_embeddings_level_0'))\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            elmo_embeddings.append(concatenate([self._model.get_layer('f_block_{}'.format(i + 1)).output,\n",
    "                                                Lambda(function=ELMo.reverse)\n",
    "                                                (self._model.get_layer('b_block_{}'.format(i + 1)).output)],\n",
    "                                               name='elmo_embeddings_level_{}'.format(i + 1)))\n",
    "\n",
    "        camos = list()\n",
    "        for i, elmo_embedding in enumerate(elmo_embeddings):\n",
    "            camos.append(Camouflage(mask_value=0.0, name='camo_elmo_embeddings_level_{}'.format(i + 1))([elmo_embedding,\n",
    "                                                                                                         self._model.get_layer(\n",
    "                                                                                                             'token_encoding').output]))\n",
    "\n",
    "        self._elmo_model = Model(inputs=[self._model.get_layer('word_indices').input], outputs=camos)\n",
    "\n",
    "        if print_summary:\n",
    "            self._elmo_model.summary()\n",
    "\n",
    "        if save:\n",
    "            self._elmo_model.save(os.path.join(MODELS_DIR, 'ELMo_Encoder.hd5'))\n",
    "            print('ELMo Encoder saved successfully')\n",
    "\n",
    "    def save(self, sampled_softmax=True):\n",
    "        \"\"\"\n",
    "        Persist model in disk\n",
    "        :param sampled_softmax: reload model using the full softmax function\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not sampled_softmax:\n",
    "            self.parameters['num_sampled'] = self.parameters['vocab_size']\n",
    "        self.compile_elmo()\n",
    "        self._model.load_weights(os.path.join(MODELS_DIR, 'elmo_best_weights.hdf5'))\n",
    "        self._model.save(os.path.join(MODELS_DIR, 'ELMo_LM_EVAL.hd5'))\n",
    "        print('ELMo Language Model saved successfully')\n",
    "\n",
    "    def load(self):\n",
    "        self._model = load_model(os.path.join(MODELS_DIR, 'ELMo_LM.h5'),\n",
    "                                 custom_objects={'TimestepDropout': TimestepDropout,\n",
    "                                                 'Camouflage': Camouflage})\n",
    "\n",
    "    def load_elmo_encoder(self):\n",
    "        self._elmo_model = load_model(os.path.join(MODELS_DIR, 'ELMo_Encoder.h5'),\n",
    "                                      custom_objects={'TimestepDropout': TimestepDropout,\n",
    "                                                      'Camouflage': Camouflage})\n",
    "\n",
    "    @staticmethod\n",
    "    def reverse(inputs, axes=1):\n",
    "        return K.reverse(inputs, axes=axes)\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(y_pred, y_true):\n",
    "\n",
    "        cross_entropies = []\n",
    "        for y_pred_seq, y_true_seq in zip(y_pred, y_true):\n",
    "            # Reshape targets to one-hot vectors\n",
    "            y_true_seq = to_categorical(y_true_seq, y_pred_seq.shape[-1])\n",
    "            # Compute cross_entropy for sentence words\n",
    "            cross_entropy = K.categorical_crossentropy(K.tf.convert_to_tensor(y_true_seq, dtype=K.tf.float32),\n",
    "                                                       K.tf.convert_to_tensor(y_pred_seq, dtype=K.tf.float32))\n",
    "            cross_entropies.extend(cross_entropy.eval(session=K.get_session()))\n",
    "\n",
    "        # Compute mean cross_entropy and perplexity\n",
    "        cross_entropy = np.mean(np.asarray(cross_entropies), axis=-1)\n",
    "\n",
    "        return pow(2.0, cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andrei/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/andrei/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/andrei/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_indices (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_encoding (Embedding)      (None, None, 200)    5782800     word_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, None, 200)    0           token_encoding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "timestep_dropout_2 (TimestepDro (None, None, 200)    0           spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 200)    0           timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, None, 400)    961600      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 200)    0           spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, None, 400)    961600      timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_7 (Camouflage)       (None, None, 400)    0           lstm_7[0][0]                     \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_5 (Camouflage)       (None, None, 400)    0           lstm_5[0][0]                     \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 200)    80200       camouflage_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 200)    80200       camouflage_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "b_block_1 (Add)                 (None, None, 200)    0           time_distributed_7[0][0]         \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "f_block_1 (Add)                 (None, None, 200)    0           time_distributed_5[0][0]         \n",
      "                                                                 timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_9 (SpatialDro (None, None, 200)    0           b_block_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, None, 200)    0           f_block_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, None, 400)    961600      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, None, 400)    961600      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_8 (Camouflage)       (None, None, 400)    0           lstm_8[0][0]                     \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_6 (Camouflage)       (None, None, 400)    0           lstm_6[0][0]                     \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 200)    80200       camouflage_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 200)    80200       camouflage_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "b_block_2 (Add)                 (None, None, 200)    0           time_distributed_8[0][0]         \n",
      "                                                                 spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "f_block_2 (Add)                 (None, None, 200)    0           time_distributed_6[0][0]         \n",
      "                                                                 spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_10 (SpatialDr (None, None, 200)    0           b_block_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, None, 200)    0           f_block_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "next_ids (InputLayer)           (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reverse (Lambda)                (None, None, 200)    0           spatial_dropout1d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "previous_ids (InputLayer)       (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sampled_softmax_2 (SampledSoftm (None, None, 200)    28914       spatial_dropout1d_8[0][0]        \n",
      "                                                                 next_ids[0][0]                   \n",
      "                                                                 reverse[0][0]                    \n",
      "                                                                 previous_ids[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 9,978,914\n",
      "Trainable params: 9,978,914\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "elmo_model = ELMo(parameters)\n",
    "elmo_model.compile_elmo(print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/andrei/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  750/18359 [>.............................] - ETA: 5:06:34 - loss: 181.8085"
     ]
    }
   ],
   "source": [
    "from tachles import LMDataGenerator\n",
    "\n",
    "train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding'])\n",
    "\n",
    "val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "\n",
    "elmo_model.train(train_data=train_generator, valid_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist ELMo Bidirectional Language Model in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model.save(sampled_softmax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Bidirectional Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ELMo meta-model to deploy for production and persist in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Use elmo with keras.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
