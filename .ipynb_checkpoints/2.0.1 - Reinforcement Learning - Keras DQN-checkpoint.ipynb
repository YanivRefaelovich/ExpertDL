{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Keras\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"img/breakout.gif?raw=true\" width=\"200\"></td>\n",
    "    <td><img src=\"img/cartpole.gif?raw=true\" width=\"200\"></td>\n",
    "    <td><img src=\"img/pendulum.gif?raw=true\" width=\"200\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "`keras-rl` implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library [Keras](http://keras.io).\n",
    "\n",
    "Furthermore, `keras-rl` works with [OpenAI Gym](https://gym.openai.com/) out of the box. This means that evaluating and playing around with different algorithms is easy.\n",
    "\n",
    "Of course you can extend `keras-rl` according to your own needs. You can use built-in Keras callbacks and metrics or define your own.\n",
    "Even more so, it is easy to implement your own environments and even algorithms by simply extending some simple abstract classes. Documentation is available [online](http://keras-rl.readthedocs.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "---\n",
    "\n",
    "### DQNAgent\n",
    "\n",
    "```python\n",
    "rl.agents.dqn.DQNAgent(model, policy=None, test_policy=None, enable_double_dqn=True, \n",
    "                       enable_dueling_network=False, dueling_type='avg')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L11)</span>\n",
    "### Agent\n",
    "\n",
    "```python\n",
    "rl.core.Agent(processor=None)\n",
    "```\n",
    "\n",
    "Abstract base class for all implemented agents.\n",
    "\n",
    "Each agent interacts with the environment (as defined by the `Env` class) by first observing the\n",
    "state of the environment. Based on this observation the agent changes the environment by performing\n",
    "an action.\n",
    "\n",
    "Do not use this abstract base class directly but instead use one of the concrete agents implemented.\n",
    "Each agent realizes a reinforcement learning algorithm. Since all agents conform to the same\n",
    "interface, you can use them interchangeably.\n",
    "\n",
    "To implement your own agent, you have to implement the following methods:\n",
    "\n",
    "- `forward`\n",
    "- `backward`\n",
    "- `compile`\n",
    "- `load_weights`\n",
    "- `save_weights`\n",
    "- `layers`\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "\n",
    "\n",
    "All agents share a common API. This allows you to easily switch between different agents.\n",
    "That being said, keep in mind that some agents make assumptions regarding the action space, i.e. assume discrete\n",
    "or continuous actions.\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L44)</span>\n",
    "\n",
    "### fit\n",
    "\n",
    "\n",
    "```python\n",
    "fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1, visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000, nb_max_episode_steps=None)\n",
    "```\n",
    "\n",
    "\n",
    "Trains the agent on the given environment.\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "- __env:__ (`Env` instance): Environment that the agent interacts with. See [Env](#env) for details.\n",
    "- __nb_steps__ (integer): Number of training steps to be performed.\n",
    "- __action_repetition__ (integer): Number of times the agent repeats the same action without\n",
    "\tobserving the environment again. Setting this to a value > 1 can be useful\n",
    "\tif a single action only has a very small effect on the environment.\n",
    "- __callbacks__ (list of `keras.callbacks.Callback` or `rl.callbacks.Callback` instances):\n",
    "\tList of callbacks to apply during training. See [callbacks](/callbacks) for details.\n",
    "- __verbose__ (integer): 0 for no logging, 1 for interval logging (compare `log_interval`), 2 for episode logging\n",
    "- __visualize__ (boolean): If `True`, the environment is visualized during training. However,\n",
    "\tthis is likely going to slow down training significantly and is thus intended to be\n",
    "\ta debugging instrument.\n",
    "- __nb_max_start_steps__ (integer): Number of maximum steps that the agent performs at the beginning\n",
    "\tof each episode using `start_step_policy`. Notice that this is an upper limit since\n",
    "\tthe exact number of steps to be performed is sampled uniformly from [0, max_start_steps]\n",
    "\tat the beginning of each episode.\n",
    "- __start_step_policy__ (`lambda observation: action`): The policy\n",
    "\tto follow if `nb_max_start_steps` > 0. If set to `None`, a random action is performed.\n",
    "- __log_interval__ (integer): If `verbose` = 1, the number of steps that are considered to be an interval.\n",
    "- __nb_max_episode_steps__ (integer): Number of steps per episode that the agent performs before\n",
    "\tautomatically resetting the environment. Set to `None` if each episode should run\n",
    "\t(potentially indefinitely) until the environment signals a terminal state.\n",
    "\n",
    "__Returns__\n",
    "\n",
    "A `keras.callbacks.History` instance that recorded the entire training process.\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L231)</span>\n",
    "\n",
    "### test\n",
    "\n",
    "\n",
    "```python\n",
    "test(self, env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True, nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1)\n",
    "```\n",
    "\n",
    "\n",
    "- __processor__ (`Processor` instance): See [Processor](#processor) for details.\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L454)</span>\n",
    "### Processor\n",
    "\n",
    "```python\n",
    "rl.core.Processor()\n",
    "```\n",
    "\n",
    "Abstract base class for implementing processors.\n",
    "\n",
    "A processor acts as a coupling mechanism between an `Agent` and its `Env`. This can\n",
    "be necessary if your agent has different requirements with respect to the form of the\n",
    "observations, actions, and rewards of the environment. By implementing a custom processor,\n",
    "you can effectively translate between the two without having to change the underlaying\n",
    "implementation of the agent or environment.\n",
    "\n",
    "Do not use this abstract base class directly but instead use one of the concrete implementations\n",
    "or write your own.\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L533)</span>\n",
    "### Env\n",
    "\n",
    "```python\n",
    "rl.core.Env()\n",
    "```\n",
    "\n",
    "The abstract environment class that is used by all agents. This class has the exact\n",
    "same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "implementation.\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"float:right;\">[[source]](https://github.com/keras-rl/keras-rl/blob/master/rl/core.py#L609)</span>\n",
    "### Space\n",
    "\n",
    "```python\n",
    "rl.core.Space()\n",
    "```\n",
    "\n",
    "Abstract model for a space that is used for the state and action spaces. This class has the\n",
    "exact same API that OpenAI Gym uses so that integrating with it is trivial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "3lOBizVa4rVt",
    "outputId": "a3142dd2-4ff0-4bb6-a833-a7046f4e0596"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "print 'Running! \\nPlease dont interrupt this cell. It might cause serious issues..'\n",
    "!pip install gym keras-rl pyglet==1.2.4 \n",
    "# !apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig \n",
    "!pip install 'gym[atari]' \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Neural Networks\n",
    "\n",
    "Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. \n",
    "\n",
    "We instead need some way to take a description of our state, and produce $Q$-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to $Q$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 $Q$-values, one for each action. \n",
    "\n",
    "Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. \n",
    "\n",
    "<img src=\"images/pong.jpg\" alt=\"\" style=\"width: 300px;\"/>\n",
    "\n",
    "The method of updating is a little different as well. \n",
    "\n",
    "Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted $Q$-values, and the “target” value is computed and the gradients passed through the network. \n",
    "\n",
    "In this case, our $Q_{target}$ for the chosen action is the equivalent to the $Q$-value computed in equation above ($\n",
    "Q(s,a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a) ]\n",
    "$).\n",
    "\n",
    "$$\n",
    "Loss = \\sum (Q_{target} - Q_{predicted})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Flatten\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the environment and extract the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-networks\n",
    "\n",
    "While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep $Q$-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
    "+ Going from a single-layer network to a multi-layer convolutional network.\n",
    "+ Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "+ Utilizing a second “target” network, which we will use to compute target $Q$-values during our updates.\n",
    "\n",
    "<img src=\"images/deepq1.png\" alt=\"\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "See https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network.\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "The second major addition to make DQNs work is Experience Replay. \n",
    "\n",
    "The problem with online learning is that the *samples arrive in order* they are experienced and as such are highly correlated. Because of this, our network will most likely overfit and fail to generalize properly.\n",
    "\n",
    "The key idea of **experience replay** is that we store these transitions in our memory and during each learning step, sample a random batch and perform a gradient descend on it. \n",
    "\n",
    "The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. \n",
    "\n",
    "### Separate Target Network\n",
    "\n",
    "This second network is used to generate the $Q$-target values that will be used to compute the loss for every action during training. \n",
    "\n",
    "The issue is that at every step of training, the $Q$-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated $Q$-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary $Q$-networks values. In this way training can proceed in a more stable manner.\n",
    "\n",
    "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(1,) + env.observation_space.shape )\n",
    "x = Flatten()(inp)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(nb_actions)(x)\n",
    "x = Activation('linear')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "even the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dual\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renderer \n",
    "for live animation in jupyter while running those command.\n",
    "in case you are running on your local device you can comment the bottom lines and and run full local environment! (It looks nicer :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Render(Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        plt.clf()\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "Ctrl + C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_steps represents the number of steps, you can try and change it\n",
    "dqn.fit(env, callbacks=[Render()], nb_steps=1000, log_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running locally, uncomment this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #nb_steps represents the number of steps, you can try and change it\n",
    "# dqn.fit(env, nb_steps=100, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we save the final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate our algorithm for 5 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, callbacks=[Render()], visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running locally, uncomment this out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA & Expected SARSA\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "$$X_t \\in \\{ v_t, \\hat{v_t} \\}$$\n",
    "\n",
    "where in expected SARSA:\n",
    "\n",
    "$$ v_t = r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)$$\n",
    "\n",
    "and in SARSA:\n",
    "\n",
    "$$ \\hat{v_t} = r_t + \\gamma Q_t (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Bias is represented by:\n",
    "\n",
    "$$Bias(s,a) = Q^{\\pi} (s,a) - E\\{X_t\\}$$\n",
    "\n",
    "Variance is denoted by:\n",
    "\n",
    "$$Var(s,a) = E\\{(X_t)^2\\} - (E\\{X_t\\})^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Activation, Flatten\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the environment and extract the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(1,) + env.observation_space.shape)\n",
    "x = Flatten()(inp)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(16)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(nb_actions)(x)\n",
    "x = Activation('linear')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA does not require a memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BoltzmannQPolicy()\n",
    "sarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\n",
    "sarsa.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "Ctrl + C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_steps represents the number of steps, you can try and change it\n",
    "dqn.fit(env, callbacks=[Render()], nb_steps=1000, log_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running locally, uncomment this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #nb_steps represents the number of steps, you can try and change it\n",
    "# sarsa.fit(env, nb_steps=10000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we save the final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate our algorithm for 5 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, callbacks=[Render()], visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running locally, uncomment this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarsa.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602), Mnih et al., 2013\n",
    "- [Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html), Mnih et al., 2015\n",
    "- [Deep Reinforcement Learning with Double Q-learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf), van Hasselt et al., 2015\n",
    "- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581), Wang et al., 2016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
